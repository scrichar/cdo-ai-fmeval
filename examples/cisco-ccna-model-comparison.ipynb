{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c367336c",
   "metadata": {},
   "source": [
    "**Summary: This notebook demonstrates how to compare two SageMaker models using the Cisco CCNA dataset for question answering evaluation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec0d89",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Cisco CCNA Dataset\n",
    "\n",
    "We'll load the Cisco CCNA dataset from HuggingFace and convert it to the format expected by fmeval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Cisco CCNA dataset\n",
    "dataset = load_dataset(\"Elfsong/Cisco_CCNA\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "print(f\"Dataset loaded with {len(df)} questions\")\n",
    "print(\"\\nDataset columns:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to fmeval format\n",
    "def convert_to_fmeval_format(df, max_records=50):\n",
    "    \"\"\"\n",
    "    Convert Cisco CCNA dataset to fmeval QA format\n",
    "    Expected format: [{\"question\": \"...\", \"answer\": \"...\"}]\n",
    "    \"\"\"\n",
    "    fmeval_data = []\n",
    "    \n",
    "    for _, row in df.head(max_records).iterrows():\n",
    "        # Use the question text and the correct answer\n",
    "        question = row['question_text']\n",
    "        correct_answer = row['correct_answer']  # This should be the letter(s) like 'A', 'B', 'BC', etc.\n",
    "        \n",
    "        # Parse the choices to get the full answer text\n",
    "        choices = row['choices']\n",
    "        \n",
    "        # Convert correct_answer letters to actual text\n",
    "        if len(correct_answer) == 1:\n",
    "            # Single answer like 'A'\n",
    "            choice_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6}\n",
    "            if correct_answer in choice_map and choice_map[correct_answer] < len(choices.split(' ')): \n",
    "                # Split choices and get the correct one\n",
    "                choice_list = [c.strip() for c in choices.split(' ') if c.strip()]\n",
    "                if choice_map[correct_answer] < len(choice_list):\n",
    "                    answer = choice_list[choice_map[correct_answer]]\n",
    "                else:\n",
    "                    answer = correct_answer\n",
    "            else:\n",
    "                answer = correct_answer\n",
    "        else:\n",
    "            # Multiple answers like 'BC' - just use the letters for now\n",
    "            answer = correct_answer\n",
    "            \n",
    "        fmeval_data.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    \n",
    "    return fmeval_data\n",
    "\n",
    "# Convert first 20 records for testing\n",
    "qa_data = convert_to_fmeval_format(df, max_records=20)\n",
    "\n",
    "print(f\"Converted {len(qa_data)} question-answer pairs\")\n",
    "print(\"\\nSample data:\")\n",
    "for i, item in enumerate(qa_data[:3]):\n",
    "    print(f\"\\nQ{i+1}: {item['question'][:100]}...\")\n",
    "    print(f\"A{i+1}: {item['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ee101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the converted data to a temporary JSONL file for fmeval\n",
    "def save_to_jsonl(data, filename):\n",
    "    \"\"\"Save data to JSONL format for fmeval\"\"\"\n",
    "    os.makedirs(\"cisco_ccna_data\", exist_ok=True)\n",
    "    filepath = os.path.join(\"cisco_ccna_data\", filename)\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Save the QA data\n",
    "qa_file_path = save_to_jsonl(qa_data, \"cisco_ccna_qa.jsonl\")\n",
    "print(f\"Saved QA data to: {qa_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd045f49",
   "metadata": {},
   "source": [
    "## 2. Configure Your SageMaker Endpoints\n",
    "\n",
    "Replace the endpoint names below with your actual SageMaker endpoint names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8184bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint names\n",
    "ENDPOINT_NAME_1 = \"jumpstart-dft-hf-llm-gemma-7b-20250813-123819\"\n",
    "ENDPOINT_NAME_2 = \"jumpstart-dft-hf-llm-gemma-7b-20250812-200929\"\n",
    "\n",
    "# Model IDs\n",
    "MODEL_ID_1 = \"gemma-7b-123819\"\n",
    "MODEL_ID_2 = \"gemma-7b-200929\"\n",
    "\n",
    "print(f\"Using endpoints:\")\n",
    "print(f\"1. {ENDPOINT_NAME_1} ({MODEL_ID_1})\")\n",
    "print(f\"2. {ENDPOINT_NAME_2} ({MODEL_ID_2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to test endpoint connectivity and determine output format\n",
    "def test_endpoint(endpoint_name, test_prompt=\"What is a router?\"):\n",
    "    \"\"\"\n",
    "    Test endpoint connectivity and determine response format\n",
    "    \"\"\"\n",
    "    try:\n",
    "        predictor = sagemaker.predictor.Predictor(\n",
    "            endpoint_name=endpoint_name,\n",
    "            serializer=sagemaker.serializers.JSONSerializer(),\n",
    "            deserializer=sagemaker.deserializers.JSONDeserializer()\n",
    "        )\n",
    "        \n",
    "        # Test with a simple payload - you may need to adjust this based on your model\n",
    "        payload = {\n",
    "            \"inputs\": test_prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 100,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = predictor.predict(payload)\n",
    "        print(f\"Endpoint {endpoint_name} is working!\")\n",
    "        print(f\"Sample response: {str(response)[:200]}...\")\n",
    "        \n",
    "        # Try to determine output format\n",
    "        if isinstance(response, list) and len(response) > 0:\n",
    "            if \"generated_text\" in response[0]:\n",
    "                output_format = \"[0].generated_text\"\n",
    "            elif \"generation\" in response[0]:\n",
    "                output_format = \"[0].generation\"\n",
    "            else:\n",
    "                output_format = \"[0]\"\n",
    "        elif isinstance(response, dict):\n",
    "            if \"generated_text\" in response:\n",
    "                output_format = \".generated_text\"\n",
    "            elif \"generation\" in response:\n",
    "                output_format = \".generation\"\n",
    "            else:\n",
    "                output_format = \"\"\n",
    "        else:\n",
    "            output_format = \"\"\n",
    "            \n",
    "        return predictor, output_format\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing endpoint {endpoint_name}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Test both endpoints\n",
    "predictor_1, output_format_1 = test_endpoint(ENDPOINT_NAME_1)\n",
    "predictor_2, output_format_2 = test_endpoint(ENDPOINT_NAME_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd629a4",
   "metadata": {},
   "source": [
    "## 3. Set up Model Runners for Evaluation\n",
    "\n",
    "We'll configure the model runners that fmeval will use to interact with your endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43dbb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fmeval.eval_algorithms.qa_accuracy import QAAccuracy, QAAccuracyConfig\n",
    "from fmeval.model_runners.sm_jumpstart_model_runner import JumpStartModelRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model runners - adjust content_template based on your model requirements\n",
    "\n",
    "# Model Runner 1\n",
    "model_runner_1 = JumpStartModelRunner(\n",
    "    endpoint_name=ENDPOINT_NAME_1,\n",
    "    model_id=MODEL_ID_1,\n",
    "    model_version=\"*\",\n",
    "    output=output_format_1,\n",
    "    content_template='{\n",
    "        \"inputs\": $prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 100,\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    }'\n",
    ")\n",
    "\n",
    "# Model Runner 2\n",
    "model_runner_2 = JumpStartModelRunner(\n",
    "    endpoint_name=ENDPOINT_NAME_2,\n",
    "    model_id=MODEL_ID_2,\n",
    "    model_version=\"*\",\n",
    "    output=output_format_2,\n",
    "    content_template='{\n",
    "        \"inputs\": $prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 100,\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    }'\n",
    ")\n",
    "\n",
    "print(\"Model runners configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e0582",
   "metadata": {},
   "source": [
    "## 4. Run the Evaluation\n",
    "\n",
    "Now we'll run the QA accuracy evaluation on both models using the Cisco CCNA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6aea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to configure and run evaluation\n",
    "def run_eval(model_runner, model_name, dataset_path):\n",
    "    \"\"\"Configure and run QA evaluation\"\"\"\n",
    "    \n",
    "    # Configure evaluation with custom dataset\n",
    "    config = QAAccuracyConfig(\n",
    "        dataset_config_name=\"cisco_ccna\",\n",
    "        dataset_name=\"cisco_ccna_qa\",\n",
    "        dataset_uri=dataset_path,\n",
    "        dataset_mime_type=\"application/jsonlines\",\n",
    "        model_outputs_to_save=None\n",
    "    )\n",
    "    \n",
    "    qa_eval = QAAccuracy(config)\n",
    "    \n",
    "    # Configure filepath for results\n",
    "    results_dir = \"cisco_ccna_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    results_path = os.path.join(results_dir, f\"{model_name}.json\")\n",
    "    \n",
    "    # Load results from file if the eval has already been run\n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "            print(f'Results loaded from {results_path}')\n",
    "    else:\n",
    "        print(f\"Running evaluation for {model_name}...\")\n",
    "        try:\n",
    "            # Run evaluation with limited records for testing\n",
    "            results = qa_eval.evaluate(\n",
    "                model=model_runner, \n",
    "                save=True,\n",
    "                num_records=10  # Start with fewer records for testing\n",
    "            )\n",
    "            \n",
    "            # Save results\n",
    "            with open(results_path, 'w') as f:\n",
    "                json.dump(results, f, default=lambda c: c.__dict__)\n",
    "                print(f'Results saved to {results_path}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error running evaluation for {model_name}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for Model 1\n",
    "if predictor_1 is not None:\n",
    "    print(f\"\\n=== Evaluating {MODEL_ID_1} ===\")\n",
    "    results_model_1 = run_eval(model_runner_1, MODEL_ID_1, qa_file_path)\n",
    "else:\n",
    "    print(f\"Skipping {MODEL_ID_1} evaluation - endpoint not available\")\n",
    "    results_model_1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3376f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for Model 2\n",
    "if predictor_2 is not None:\n",
    "    print(f\"\\n=== Evaluating {MODEL_ID_2} ===\")\n",
    "    results_model_2 = run_eval(model_runner_2, MODEL_ID_2, qa_file_path)\n",
    "else:\n",
    "    print(f\"Skipping {MODEL_ID_2} evaluation - endpoint not available\")\n",
    "    results_model_2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6dfe82",
   "metadata": {},
   "source": [
    "## 5. Visualize Results\n",
    "\n",
    "We'll create radar plots to compare the performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cfc1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install plotting packages if not already available\n",
    "!pip install -U plotly kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and format results for visualization\n",
    "def load_results_for_viz(model_names):\n",
    "    \"\"\"Load evaluation results and format for visualization\"\"\"\n",
    "    accuracy_results = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        results_path = os.path.join(\"cisco_ccna_results\", f\"{model_name}.json\")\n",
    "        \n",
    "        if not os.path.exists(results_path):\n",
    "            print(f\"Results file not found: {results_path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(results_path, 'r') as f:\n",
    "                res = json.load(f)\n",
    "                \n",
    "            for accuracy_eval in res:\n",
    "                for accuracy_scores in accuracy_eval[\"dataset_scores\"]:\n",
    "                    accuracy_results.append({\n",
    "                        'model': model_name,\n",
    "                        'evaluation': 'accuracy',\n",
    "                        'dataset': accuracy_eval[\"dataset_name\"],\n",
    "                        'metric': accuracy_scores[\"name\"],\n",
    "                        'value': accuracy_scores[\"value\"]\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading results for {model_name}: {str(e)}\")\n",
    "            \n",
    "    return pd.DataFrame(accuracy_results)\n",
    "\n",
    "# Function to create radar plot\n",
    "def visualize_cisco_results(results_df):\n",
    "    \"\"\"Create radar plot for Cisco CCNA evaluation results\"\"\"\n",
    "    if results_df.empty:\n",
    "        print(\"No results available for visualization\")\n",
    "        return\n",
    "        \n",
    "    # Create the radar plot\n",
    "    fig = px.line_polar(\n",
    "        results_df, \n",
    "        r='value', \n",
    "        theta='metric', \n",
    "        color='model', \n",
    "        line_close=True,\n",
    "        title=\"Model Comparison on Cisco CCNA Dataset\"\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1]\n",
    "            )\n",
    "        ),\n",
    "        title=dict(font=dict(size=20)),\n",
    "        margin=dict(l=150, r=0, t=100, b=80)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    results_dir = \"cisco_ccna_results\"\n",
    "    fig.write_image(os.path.join(results_dir, \"cisco_ccna_comparison.pdf\"))\n",
    "    fig.write_html(os.path.join(results_dir, \"cisco_ccna_comparison.html\"))\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4059fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize results\n",
    "available_models = []\n",
    "if results_model_1 is not None:\n",
    "    available_models.append(MODEL_ID_1)\n",
    "if results_model_2 is not None:\n",
    "    available_models.append(MODEL_ID_2)\n",
    "\n",
    "if available_models:\n",
    "    print(f\"Loading results for models: {available_models}\")\n",
    "    results_df = load_results_for_viz(available_models)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\nResults summary:\")\n",
    "        print(results_df.groupby(['model', 'metric'])['value'].mean())\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = visualize_cisco_results(results_df)\n",
    "    else:\n",
    "        print(\"No valid results found for visualization\")\n",
    "else:\n",
    "    print(\"No models were successfully evaluated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e840cb",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "This notebook provides a framework for comparing your SageMaker models using the Cisco CCNA dataset. \n",
    "\n",
    "### Key Points:\n",
    "1. **Dataset**: Uses the Cisco CCNA networking questions dataset\n",
    "2. **Models**: Compares two of your existing SageMaker endpoints\n",
    "3. **Evaluation**: Uses fmeval's QA accuracy metrics\n",
    "4. **Visualization**: Creates radar plots to compare performance\n",
    "\n",
    "### To customize for your use case:\n",
    "1. Update the endpoint names and model IDs\n",
    "2. Adjust the content templates based on your model's expected input format\n",
    "3. Modify the output format parsing if needed\n",
    "4. Increase the number of evaluation records once you verify everything works\n",
    "\n",
    "### Troubleshooting:\n",
    "- If endpoints fail to connect, verify they're running and accessible\n",
    "- If evaluation fails, check the content template format matches your model's API\n",
    "- Adjust the output format string based on your model's response structure"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
