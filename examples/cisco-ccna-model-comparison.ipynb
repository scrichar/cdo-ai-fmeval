{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install deps\n",
    "!pip install fmeval plotly datasets transformers kaleido\n",
    "\n",
    "print(\"ðŸ’¡ Alternative installation method available above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"ðŸ” Testing imports...\")\n",
    "\n",
    "# Test core imports\n",
    "try:\n",
    "    print(f\"âœ… pandas: {pd.__version__}\")\n",
    "    print(f\"âœ… sagemaker: {sagemaker.__version__}\")\n",
    "    print(\"âœ… datasets: imported successfully\")\n",
    "    print(\"âœ… os and json: built-in modules\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Core import failed: {e}\")\n",
    "\n",
    "# Try to import fmeval to verify installation\n",
    "try:\n",
    "    import fmeval\n",
    "    print(f\"âœ… fmeval imported successfully\")\n",
    "    try:\n",
    "        print(f\"   Version: {fmeval.__version__}\")\n",
    "    except AttributeError:\n",
    "        print(\"   Version: unknown\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ fmeval import failed: {e}\")\n",
    "    print(\"ðŸ’¡ Please run the installation cell above, or try:\")\n",
    "    print(\"   !pip install -e /opt/ml/code\")\n",
    "\n",
    "# Test optional imports\n",
    "try:\n",
    "    import plotly\n",
    "    print(f\"âœ… plotly: {plotly.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  plotly not available - install with: !pip install plotly\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Current working directory: {os.getcwd()}\")\n",
    "print(f\"ðŸ Python executable: {sys.executable}\")\n",
    "print(f\"ðŸ”§ Python version: {sys.version}\")\n",
    "\n",
    "# Check if we're in SageMaker Studio\n",
    "if '/opt/ml' in os.getcwd() or 'sagemaker-user' in os.getcwd():\n",
    "    print(\"ðŸ­ Running in SageMaker Studio environment\")\n",
    "else:\n",
    "    print(\"ðŸ’» Running in local development environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c367336c",
   "metadata": {},
   "source": [
    "# ðŸ“Š Cisco CCNA Model Comparison Notebook\n",
    "\n",
    "**Summary: This notebook demonstrates how to compare two SageMaker models using the Cisco CCNA dataset for question answering evaluation.**\n",
    "\n",
    "## ðŸš€ Setup Instructions for SageMaker Studio\n",
    "\n",
    "### **1. Package Installation Process:**\n",
    "- **First time**: Run the package installation cell above\n",
    "- **Kernel restart**: If you restart the kernel, re-run the installation cell\n",
    "- **Persistent installs**: Packages install to the kernel environment and persist across sessions\n",
    "\n",
    "### **2. Common SageMaker Studio Installation Patterns:**\n",
    "```python\n",
    "# Pattern 1: Direct pip install (most common)\n",
    "!pip install package_name\n",
    "\n",
    "# Pattern 2: Install from source (for fmeval)\n",
    "!pip install -e /opt/ml/code\n",
    "\n",
    "# Pattern 3: Install with specific versions\n",
    "!pip install \"package>=1.0.0\"\n",
    "\n",
    "# Pattern 4: Install multiple packages\n",
    "!pip install package1 package2 package3\n",
    "```\n",
    "\n",
    "### **3. Kernel Management:**\n",
    "- **Image**: Use `Python 3` or `Data Science` kernel image\n",
    "- **Instance**: `ml.t3.medium` or larger for better performance\n",
    "- **Restart**: Kernel â†’ Restart Kernel if you encounter import issues\n",
    "\n",
    "### **4. File Locations in SageMaker Studio:**\n",
    "- **Notebooks**: `/home/sagemaker-user/`\n",
    "- **Code**: `/opt/ml/code` (if uploaded via git/code repo)\n",
    "- **Data**: Can be in S3 or local `/tmp/` directory\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec0d89",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Cisco CCNA Dataset\n",
    "\n",
    "We'll load the Cisco CCNA dataset from HuggingFace and convert it to the format expected by fmeval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Cisco CCNA dataset\n",
    "dataset = load_dataset(\"Elfsong/Cisco_CCNA\")\n",
    "\n",
    "# Check available splits\n",
    "print(\"Available splits:\", list(dataset.keys()))\n",
    "\n",
    "# Try different ways to access the data\n",
    "if 'train' in dataset:\n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "elif 'volume1' in dataset:\n",
    "    # Combine all volumes if that's how the data is structured\n",
    "    all_data = []\n",
    "    for split_name in dataset.keys():\n",
    "        if split_name.startswith('volume'):\n",
    "            all_data.extend(dataset[split_name])\n",
    "    df = pd.DataFrame(all_data)\n",
    "else:\n",
    "    # Use first available split\n",
    "    first_split = list(dataset.keys())[0]\n",
    "    df = pd.DataFrame(dataset[first_split])\n",
    "\n",
    "print(f\"Dataset loaded with {len(df)} questions\")\n",
    "print(\"\\nDataset columns:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to fmeval format\n",
    "def convert_to_fmeval_format(df, max_records=50):\n",
    "    \"\"\"\n",
    "    Convert Cisco CCNA dataset to fmeval QA format.\n",
    "    Expected format: [{\"question\": \"...\", \"answer\": \"...\"}]\n",
    "    \"\"\"\n",
    "    fmeval_data = []\n",
    "    \n",
    "    for _, row in df.head(max_records).iterrows():\n",
    "        # Use the Question and Answer columns from the actual dataset\n",
    "        question = row['Question']\n",
    "        correct_answer = row['Answer']  # This should be the letter(s) like 'A', 'B', 'DF', etc.\n",
    "        choices = row['Choices']\n",
    "        \n",
    "        # Parse the choices to get the full answer text\n",
    "        choice_lines = [line.strip() for line in choices.split('\\n') if line.strip()]\n",
    "        choice_dict = {}\n",
    "        \n",
    "        for choice_line in choice_lines:\n",
    "            if choice_line and len(choice_line) > 2 and choice_line[1] == '.':\n",
    "                letter = choice_line[0]\n",
    "                text = choice_line[2:].strip()\n",
    "                choice_dict[letter] = text\n",
    "        \n",
    "        # Convert correct_answer letters to actual text\n",
    "        answer_texts = []\n",
    "        for letter in correct_answer:\n",
    "            if letter in choice_dict:\n",
    "                answer_texts.append(choice_dict[letter])\n",
    "        \n",
    "        if answer_texts:\n",
    "            answer = '; '.join(answer_texts)  # Join multiple answers with semicolon\n",
    "        else:\n",
    "            answer = correct_answer  # Fallback to the letter(s)\n",
    "        \n",
    "        fmeval_data.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    \n",
    "    return fmeval_data\n",
    "\n",
    "# Convert first 20 records for testing\n",
    "qa_data = convert_to_fmeval_format(df, max_records=20)\n",
    "\n",
    "print(f\"Converted {len(qa_data)} question-answer pairs\")\n",
    "print(\"\\nSample data:\")\n",
    "for i in range(min(3, len(qa_data))):\n",
    "    print(f\"\\nQ{i+1}: {qa_data[i]['question'][:100]}...\")\n",
    "    print(f\"A{i+1}: {qa_data[i]['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ee101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the converted data to a temporary JSONL file for fmeval\n",
    "def save_to_jsonl(data, filename):\n",
    "    \"\"\"Save data to JSONL format for fmeval\"\"\"\n",
    "    os.makedirs(\"cisco_ccna_data\", exist_ok=True)\n",
    "    filepath = os.path.join(\"cisco_ccna_data\", filename)\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Save the QA data\n",
    "qa_file_path = save_to_jsonl(qa_data, \"cisco_ccna_qa.jsonl\")\n",
    "print(f\"Saved QA data to: {qa_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd045f49",
   "metadata": {},
   "source": [
    "## 2. Configure Your SageMaker Endpoints\n",
    "\n",
    "Replace the endpoint names below with your actual SageMaker endpoint names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8184bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint names\n",
    "ENDPOINT_NAME_1 = \"jumpstart-dft-hf-llm-gemma-7b-20250813-123819\"\n",
    "ENDPOINT_NAME_2 = \"jumpstart-dft-hf-llm-gemma-7b-20250812-200929\"\n",
    "\n",
    "# Model IDs\n",
    "MODEL_ID_1 = \"gemma-7b-123819\"\n",
    "MODEL_ID_2 = \"gemma-7b-200929\"\n",
    "\n",
    "print(f\"Using endpoints:\")\n",
    "print(f\"1. {ENDPOINT_NAME_1} ({MODEL_ID_1})\")\n",
    "print(f\"2. {ENDPOINT_NAME_2} ({MODEL_ID_2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to test endpoint connectivity and determine output format\n",
    "def test_endpoint(endpoint_name, test_prompt=\"What is a router?\"):\n",
    "    \"\"\"\n",
    "    Test endpoint connectivity and determine response format\n",
    "    \"\"\"\n",
    "    try:\n",
    "        predictor = sagemaker.predictor.Predictor(\n",
    "            endpoint_name=endpoint_name,\n",
    "            serializer=sagemaker.serializers.JSONSerializer(),\n",
    "            deserializer=sagemaker.deserializers.JSONDeserializer()\n",
    "        )\n",
    "        \n",
    "        # Test with a simple payload - you may need to adjust this based on your model\n",
    "        payload = {\n",
    "            \"inputs\": test_prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 100,\n",
    "                \"temperature\": 0.1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = predictor.predict(payload)\n",
    "        print(f\"Endpoint {endpoint_name} is working!\")\n",
    "        print(f\"Sample response: {str(response)[:200]}...\")\n",
    "        \n",
    "        # Try to determine output format\n",
    "        if isinstance(response, list) and len(response) > 0:\n",
    "            if \"generated_text\" in response[0]:\n",
    "                output_format = \"[0].generated_text\"\n",
    "            elif \"generation\" in response[0]:\n",
    "                output_format = \"[0].generation\"\n",
    "            else:\n",
    "                output_format = \"[0]\"\n",
    "        elif isinstance(response, dict):\n",
    "            if \"generated_text\" in response:\n",
    "                output_format = \".generated_text\"\n",
    "            elif \"generation\" in response:\n",
    "                output_format = \".generation\"\n",
    "            else:\n",
    "                output_format = \"\"\n",
    "        else:\n",
    "            output_format = \"\"\n",
    "            \n",
    "        return predictor, output_format\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing endpoint {endpoint_name}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Test both endpoints\n",
    "predictor_1, output_format_1 = test_endpoint(ENDPOINT_NAME_1)\n",
    "predictor_2, output_format_2 = test_endpoint(ENDPOINT_NAME_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd629a4",
   "metadata": {},
   "source": [
    "## 3. Set up Model Runners for Evaluation\n",
    "\n",
    "We'll configure the model runners that fmeval will use to interact with your endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43dbb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fmeval.eval_algorithms.qa_accuracy import QAAccuracy, QAAccuracyConfig\n",
    "from fmeval.model_runners.sm_jumpstart_model_runner import JumpStartModelRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model runners - adjust content_template based on your model requirements\n",
    "\n",
    "# Model Runner 1\n",
    "model_runner_1 = JumpStartModelRunner(\n",
    "    endpoint_name=ENDPOINT_NAME_1,\n",
    "    model_id=MODEL_ID_1,\n",
    "    model_version=\"*\",\n",
    "    output=output_format_1,\n",
    "    content_template='{\n",
    "        \"inputs\": $prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 100,\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    }'\n",
    ")\n",
    "\n",
    "# Model Runner 2\n",
    "model_runner_2 = JumpStartModelRunner(\n",
    "    endpoint_name=ENDPOINT_NAME_2,\n",
    "    model_id=MODEL_ID_2,\n",
    "    model_version=\"*\",\n",
    "    output=output_format_2,\n",
    "    content_template='{\n",
    "        \"inputs\": $prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 100,\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    }'\n",
    ")\n",
    "\n",
    "print(\"Model runners configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e0582",
   "metadata": {},
   "source": [
    "## 4. Run the Evaluation\n",
    "\n",
    "Now we'll run the QA accuracy evaluation on both models using the Cisco CCNA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6aea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to configure and run evaluation\n",
    "def run_eval(model_runner, model_name, dataset_path):\n",
    "    \"\"\"Configure and run QA evaluation\"\"\"\n",
    "    \n",
    "    # Configure evaluation with custom dataset\n",
    "    config = QAAccuracyConfig(\n",
    "        dataset_config_name=\"cisco_ccna\",\n",
    "        dataset_name=\"cisco_ccna_qa\",\n",
    "        dataset_uri=dataset_path,\n",
    "        dataset_mime_type=\"application/jsonlines\",\n",
    "        model_outputs_to_save=None\n",
    "    )\n",
    "    \n",
    "    qa_eval = QAAccuracy(config)\n",
    "    \n",
    "    # Configure filepath for results\n",
    "    results_dir = \"cisco_ccna_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    results_path = os.path.join(results_dir, f\"{model_name}.json\")\n",
    "    \n",
    "    # Load results from file if the eval has already been run\n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "            print(f'Results loaded from {results_path}')\n",
    "    else:\n",
    "        print(f\"Running evaluation for {model_name}...\")\n",
    "        try:\n",
    "            # Run evaluation with limited records for testing\n",
    "            results = qa_eval.evaluate(\n",
    "                model=model_runner, \n",
    "                save=True,\n",
    "                num_records=10  # Start with fewer records for testing\n",
    "            )\n",
    "            \n",
    "            # Save results\n",
    "            with open(results_path, 'w') as f:\n",
    "                json.dump(results, f, default=lambda c: c.__dict__)\n",
    "                print(f'Results saved to {results_path}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error running evaluation for {model_name}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for Model 1\n",
    "if predictor_1 is not None:\n",
    "    print(f\"\\n=== Evaluating {MODEL_ID_1} ===\")\n",
    "    results_model_1 = run_eval(model_runner_1, MODEL_ID_1, qa_file_path)\n",
    "else:\n",
    "    print(f\"Skipping {MODEL_ID_1} evaluation - endpoint not available\")\n",
    "    results_model_1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3376f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for Model 2\n",
    "if predictor_2 is not None:\n",
    "    print(f\"\\n=== Evaluating {MODEL_ID_2} ===\")\n",
    "    results_model_2 = run_eval(model_runner_2, MODEL_ID_2, qa_file_path)\n",
    "else:\n",
    "    print(f\"Skipping {MODEL_ID_2} evaluation - endpoint not available\")\n",
    "    results_model_2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6dfe82",
   "metadata": {},
   "source": [
    "## 5. Visualize Results\n",
    "\n",
    "We'll create radar plots to compare the performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cfc1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install plotting packages if not already available\n",
    "!pip install -U plotly kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and format results for visualization\n",
    "def load_results_for_viz(model_names):\n",
    "    \"\"\"Load evaluation results and format for visualization\"\"\"\n",
    "    accuracy_results = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        results_path = os.path.join(\"cisco_ccna_results\", f\"{model_name}.json\")\n",
    "        \n",
    "        if not os.path.exists(results_path):\n",
    "            print(f\"Results file not found: {results_path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(results_path, 'r') as f:\n",
    "                res = json.load(f)\n",
    "                \n",
    "            for accuracy_eval in res:\n",
    "                for accuracy_scores in accuracy_eval[\"dataset_scores\"]:\n",
    "                    accuracy_results.append({\n",
    "                        'model': model_name,\n",
    "                        'evaluation': 'accuracy',\n",
    "                        'dataset': accuracy_eval[\"dataset_name\"],\n",
    "                        'metric': accuracy_scores[\"name\"],\n",
    "                        'value': accuracy_scores[\"value\"]\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading results for {model_name}: {str(e)}\")\n",
    "            \n",
    "    return pd.DataFrame(accuracy_results)\n",
    "\n",
    "# Function to create radar plot\n",
    "def visualize_cisco_results(results_df):\n",
    "    \"\"\"Create radar plot for Cisco CCNA evaluation results\"\"\"\n",
    "    if results_df.empty:\n",
    "        print(\"No results available for visualization\")\n",
    "        return\n",
    "        \n",
    "    # Create the radar plot\n",
    "    fig = px.line_polar(\n",
    "        results_df, \n",
    "        r='value', \n",
    "        theta='metric', \n",
    "        color='model', \n",
    "        line_close=True,\n",
    "        title=\"Model Comparison on Cisco CCNA Dataset\"\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1]\n",
    "            )\n",
    "        ),\n",
    "        title=dict(font=dict(size=20)),\n",
    "        margin=dict(l=150, r=0, t=100, b=80)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    results_dir = \"cisco_ccna_results\"\n",
    "    fig.write_image(os.path.join(results_dir, \"cisco_ccna_comparison.pdf\"))\n",
    "    fig.write_html(os.path.join(results_dir, \"cisco_ccna_comparison.html\"))\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4059fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize results\n",
    "available_models = []\n",
    "if results_model_1 is not None:\n",
    "    available_models.append(MODEL_ID_1)\n",
    "if results_model_2 is not None:\n",
    "    available_models.append(MODEL_ID_2)\n",
    "\n",
    "if available_models:\n",
    "    print(f\"Loading results for models: {available_models}\")\n",
    "    results_df = load_results_for_viz(available_models)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\nResults summary:\")\n",
    "        print(results_df.groupby(['model', 'metric'])['value'].mean())\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = visualize_cisco_results(results_df)\n",
    "    else:\n",
    "        print(\"No valid results found for visualization\")\n",
    "else:\n",
    "    print(\"No models were successfully evaluated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e840cb",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "This notebook provides a framework for comparing your SageMaker models using the Cisco CCNA dataset. \n",
    "\n",
    "### Key Points:\n",
    "1. **Dataset**: Uses the Cisco CCNA networking questions dataset\n",
    "2. **Models**: Compares two of your existing SageMaker endpoints\n",
    "3. **Evaluation**: Uses fmeval's QA accuracy metrics\n",
    "4. **Visualization**: Creates radar plots to compare performance\n",
    "\n",
    "### To customize for your use case:\n",
    "1. Update the endpoint names and model IDs\n",
    "2. Adjust the content templates based on your model's expected input format\n",
    "3. Modify the output format parsing if needed\n",
    "4. Increase the number of evaluation records once you verify everything works\n",
    "\n",
    "### Troubleshooting:\n",
    "- If endpoints fail to connect, verify they're running and accessible\n",
    "- If evaluation fails, check the content template format matches your model's API\n",
    "- Adjust the output format string based on your model's response structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
